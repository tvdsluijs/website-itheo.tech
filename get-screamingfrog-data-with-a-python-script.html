<!DOCTYPE html><html lang="en-gb"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Get ScreamingFrog Data with a Python script - iTheo.Tech</title><meta name="description" content="The last few days I have been writing about getting URL data from a site with python. A few years back I started to write a python script to get URL data to analyze as ScreamingFrog does. At a certain point, I lost track of&hellip;"><meta name="generator" content="Publii Open-Source CMS for Static Site"><link rel="canonical" href="https://itheo.tech/get-screamingfrog-data-with-a-python-script.html"><link rel="alternate" type="application/atom+xml" href="https://itheo.tech/feed.xml"><link rel="alternate" type="application/json" href="https://itheo.tech/feed.json"><meta property="og:title" content="Get ScreamingFrog Data with a Python script"><meta property="og:image" content="https://itheo.tech/media/posts/68/561942551cd7c6ce293704ddb9b66507.jpeg"><meta property="og:image:width" content="1600"><meta property="og:image:height" content="1067"><meta property="og:site_name" content="iTheo.Tech"><meta property="og:description" content="The last few days I have been writing about getting URL data from a site with python. A few years back I started to write a python script to get URL data to analyze as ScreamingFrog does. At a certain point, I lost track of&hellip;"><meta property="og:url" content="https://itheo.tech/get-screamingfrog-data-with-a-python-script.html"><meta property="og:type" content="article"><link rel="stylesheet" href="https://itheo.tech/assets/css/style.css?v=6fbb1e8931a5afe843374fd67c192c86"><script type="application/ld+json">{"@context":"http://schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"https://itheo.tech/get-screamingfrog-data-with-a-python-script.html"},"headline":"Get ScreamingFrog Data with a Python script","datePublished":"2023-04-06T04:16","dateModified":"2024-06-01T19:42","image":{"@type":"ImageObject","url":"https://itheo.tech/media/posts/68/561942551cd7c6ce293704ddb9b66507.jpeg","height":1067,"width":1600},"description":"The last few days I have been writing about getting URL data from a site with python. A few years back I started to write a python script to get URL data to analyze as ScreamingFrog does. At a certain point, I lost track of&hellip;","author":{"@type":"Person","name":"Theo van der Sluijs","url":"https://itheo.tech/authors/theo-van-der-sluijs/"},"publisher":{"@type":"Organization","name":"Theo van der Sluijs","logo":{"@type":"ImageObject","url":"https://itheo.tech/media/website/itheo.tech-500x125-logo.png","height":125,"width":500}}}</script><noscript><style>img[loading] {
                    opacity: 1;
                }</style></noscript></head><body><div class="site-container"><header class="top" id="js-header"><a class="logo" href="https://itheo.tech/"><img src="https://itheo.tech/media/website/itheo.tech-500x125-logo.png" alt="iTheo.Tech" width="500" height="125"></a><div class="search"><div class="search__overlay js-search-overlay"><div class="search__overlay-inner"><button class="search__close js-search-close" aria-label="Close">Close</button></div></div><button class="search__btn js-search-btn" aria-label="Search"><svg role="presentation" focusable="false"><use xlink:href="https://itheo.tech/assets/svg/svg-map.svg#search"/></svg></button></div></header><main><article class="post"><div class="hero"><figure class="hero__image hero__image--overlay"><img src="https://itheo.tech/media/posts/68/561942551cd7c6ce293704ddb9b66507.jpeg" srcset="https://itheo.tech/media/posts/68/responsive/561942551cd7c6ce293704ddb9b66507-xs.webp 300w, https://itheo.tech/media/posts/68/responsive/561942551cd7c6ce293704ddb9b66507-sm.webp 480w, https://itheo.tech/media/posts/68/responsive/561942551cd7c6ce293704ddb9b66507-md.webp 768w, https://itheo.tech/media/posts/68/responsive/561942551cd7c6ce293704ddb9b66507-lg.webp 1024w, https://itheo.tech/media/posts/68/responsive/561942551cd7c6ce293704ddb9b66507-xl.webp 1360w, https://itheo.tech/media/posts/68/responsive/561942551cd7c6ce293704ddb9b66507-2xl.webp 1600w" sizes="(max-width: 1600px) 100vw, 1600px" loading="eager" height="1067" width="1600" alt=""></figure><header class="hero__content"><div class="wrapper"><div class="post__meta"><time datetime="2023-04-06T04:16">April 6, 2023</time></div><h1>Get ScreamingFrog Data with a Python script</h1><div class="post__meta post__meta--author"><img src="https://itheo.tech/media/website/64.png" loading="eager" height="512" width="512" class="post__author-thumb" alt="Theo van der Sluijs"> <a href="https://itheo.tech/authors/theo-van-der-sluijs/" class="feed__author">Theo van der Sluijs</a></div></div></header></div><div class="wrapper post__entry"><p>The last few days I have been writing about getting URL data from a site with python.</p><p>A few years back I started to write a python script to get URL data to analyze as ScreamingFrog does. At a certain point, I lost track of the script and figured that it would be easier to just use ScreamingFrog.</p><p>Last week I had to renew my screaming frog subscription (you pay per year and cannot use until you renew your yearly Subscription again) but the price went up from 140 to 199 !!</p><p>So it got me started thinking about the python script again, and I started writing again, with a little help from chatGPT</p><p>So last night I was thinking, let's try to combine more Url information in one Excel file.</p><p>So the assignment is:</p><blockquote><p>Create a python script with which you can recursively scan all URLs of a site, do not scan images, videos, javascript or css files. You put the results in a xslt file called "url-scan.xlst" In the file you put per url:<br>in the first column url,<br>in the next column the response code,<br>in the next column the title,<br>in the next column the number of characters of the title,<br>in the next column the number of characters of the meta description,<br>in the next column the number of H1 tags,<br>in the next column if the title is equal to the first H1 tag with True or False,<br>in the next column the number of H2 tags,<br>in the next column the number of H3 tags,<br>in the next column the word count of the content,<br>in the next column the number of urls via HTTPS,<br>in the next column the number of urls via HTTP,<br>in the next column the number of images in the page without an ALT attribute,<br>in the next column the number of images in the page without an ALT title<br>You do not scan urls outside the site.<br>Ask for the base_url with input and check if it is a valid url</p></blockquote><p>And off we go!</p><p>To create a Python script that accomplishes these tasks, you can use the following code. This script uses the <code>requests</code>, <code>BeautifulSoup</code>, and <code>lxml</code> libraries. Please make sure to install them using pip before running the script:</p><pre><code class="lang-bash">pip install requests beautifulsoup4 lxml openpyxl<restore-line-break></restore-line-break></code></pre><p>Here's the Python script:</p><pre><code class="lang-python"><span class="hljs-keyword">import</span> requests<restore-line-break><span class="hljs-keyword">from</span> bs4 <span class="hljs-keyword">import</span> BeautifulSoup<restore-line-break><span class="hljs-keyword">from</span> urllib.parse <span class="hljs-keyword">import</span> urlparse, urljoin<restore-line-break><span class="hljs-keyword">import</span> re<restore-line-break><span class="hljs-keyword">import</span> openpyxl<restore-line-break><span class="hljs-keyword">from</span> openpyxl <span class="hljs-keyword">import</span> Workbook<restore-line-break><restore-line-break><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">is_valid_url</span>(<span class="hljs-params">url</span>):</span><restore-line-break>    <span class="hljs-keyword">try</span>:<restore-line-break>        parsed_url = urlparse(url)<restore-line-break>        <span class="hljs-keyword">return</span> all([parsed_url.scheme, parsed_url.netloc])<restore-line-break>    <span class="hljs-keyword">except</span> ValueError:<restore-line-break>        <span class="hljs-keyword">return</span> <span class="hljs-literal">False</span><restore-line-break><restore-line-break><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">is_internal_url</span>(<span class="hljs-params">base_url, url</span>):</span><restore-line-break>    <span class="hljs-keyword">return</span> urlparse(url).netloc == urlparse(base_url).netloc<restore-line-break><restore-line-break><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_all_links</span>(<span class="hljs-params">url, soup</span>):</span><restore-line-break>    links = set()<restore-line-break>    <span class="hljs-keyword">for</span> a_tag <span class="hljs-keyword">in</span> soup.find_all(<span class="hljs-string">"a"</span>):<restore-line-break>        href = a_tag.attrs.get(<span class="hljs-string">"href"</span>)<restore-line-break>        <span class="hljs-keyword">if</span> href == <span class="hljs-string">""</span> <span class="hljs-keyword">or</span> href <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<restore-line-break>            <span class="hljs-keyword">continue</span><restore-line-break><restore-line-break>        full_url = urljoin(url, href)<restore-line-break>        <span class="hljs-keyword">if</span> is_internal_url(url, full_url) <span class="hljs-keyword">and</span> is_valid_url(full_url) <span class="hljs-keyword">and</span> <span class="hljs-keyword">not</span> any(re.match(pattern, full_url) <span class="hljs-keyword">for</span> pattern <span class="hljs-keyword">in</span> ignore_patterns):<restore-line-break>            links.add(full_url)<restore-line-break><restore-line-break>    <span class="hljs-keyword">return</span> links<restore-line-break><restore-line-break><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">analyze_url</span>(<span class="hljs-params">url</span>):</span><restore-line-break>    response = requests.get(url)<restore-line-break>    soup = BeautifulSoup(response.content, <span class="hljs-string">"lxml"</span>)<restore-line-break><restore-line-break>    title_tag = soup.find(<span class="hljs-string">"title"</span>)<restore-line-break>    title = title_tag.text <span class="hljs-keyword">if</span> title_tag <span class="hljs-keyword">else</span> <span class="hljs-string">""</span><restore-line-break>    title_count = len(title)<restore-line-break><restore-line-break>    meta_description = soup.find(<span class="hljs-string">"meta"</span>, attrs={<span class="hljs-string">"name"</span>: <span class="hljs-string">"description"</span>})<restore-line-break>    meta_description_count = len(meta_description[<span class="hljs-string">"content"</span>]) <span class="hljs-keyword">if</span> meta_description <span class="hljs-keyword">and</span> <span class="hljs-string">"content"</span> <span class="hljs-keyword">in</span> meta_description.attrs <span class="hljs-keyword">else</span> <span class="hljs-number">0</span><restore-line-break><restore-line-break>    h1_tags = soup.find_all(<span class="hljs-string">"h1"</span>)<restore-line-break>    h1_count = len(h1_tags)<restore-line-break>    first_h1 = h1_tags[<span class="hljs-number">0</span>].text <span class="hljs-keyword">if</span> h1_tags <span class="hljs-keyword">else</span> <span class="hljs-string">""</span><restore-line-break>    title_equals_first_h1 = title == first_h1<restore-line-break><restore-line-break>    h2_count = len(soup.find_all(<span class="hljs-string">"h2"</span>))<restore-line-break>    h3_count = len(soup.find_all(<span class="hljs-string">"h3"</span>))<restore-line-break><restore-line-break>    content = soup.body.text <span class="hljs-keyword">if</span> soup.body <span class="hljs-keyword">else</span> <span class="hljs-string">""</span><restore-line-break>    word_count = len(content.split())<restore-line-break><restore-line-break>    https_links = sum(<span class="hljs-number">1</span> <span class="hljs-keyword">for</span> link <span class="hljs-keyword">in</span> get_all_links(url, soup) <span class="hljs-keyword">if</span> urlparse(link).scheme == <span class="hljs-string">"https"</span>)<restore-line-break>    http_links = sum(<span class="hljs-number">1</span> <span class="hljs-keyword">for</span> link <span class="hljs-keyword">in</span> get_all_links(url, soup) <span class="hljs-keyword">if</span> urlparse(link).scheme == <span class="hljs-string">"http"</span>)<restore-line-break><restore-line-break>    img_tags = soup.find_all(<span class="hljs-string">"img"</span>)<restore-line-break>    img_without_alt = sum(<span class="hljs-number">1</span> <span class="hljs-keyword">for</span> img <span class="hljs-keyword">in</span> img_tags <span class="hljs-keyword">if</span> <span class="hljs-string">"alt"</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> img.attrs)<restore-line-break>    img_without_title = sum(<span class="hljs-number">1</span> <span class="hljs-keyword">for</span> img <span class="hljs-keyword">in</span> img_tags <span class="hljs-keyword">if</span> <span class="hljs-string">"title"</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> img.attrs)<restore-line-break><restore-line-break>    <span class="hljs-keyword">return</span> [<restore-line-break>        url, response.status_code, title, title_count, meta_description_count, h1_count, title_equals_first_h1,<restore-line-break>        h2_count, h3_count, word_count, https_links, http_links, img_without_alt, img_without_title<restore-line-break>    ]<restore-line-break><restore-line-break><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">crawl</span>(<span class="hljs-params">url, max_depth=<span class="hljs-number">5</span>, depth=<span class="hljs-number">1</span></span>):</span><restore-line-break>    <span class="hljs-keyword">global</span> visited_urls<restore-line-break>    <span class="hljs-keyword">global</span> results<restore-line-break>    <span class="hljs-keyword">if</span> depth > max_depth <span class="hljs-keyword">or</span> url <span class="hljs-keyword">in</span> visited_urls:<restore-line-break>        <span class="hljs-keyword">return</span><restore-line-break><restore-line-break>    visited_urls.add(url)<restore-line-break>    <span class="hljs-keyword">try</span>:<restore-line-break>        result = analyze_url(url)<restore-line-break>        results.append(result)<restore-line-break>        print(<span class="hljs-string">f"[<span class="hljs-subst">{depth}</span>] <span class="hljs-subst">{url}</span>"</span>)<restore-line-break><restore-line-break>        soup = BeautifulSoup(requests.get(url).content, <span class="hljs-string">"lxml"</span>)<restore-line-break>        <span class="hljs-keyword">for</span> link <span class="hljs-keyword">in</span> get_all_links(url, soup):<restore-line-break>            crawl(link, max_depth, depth + <span class="hljs-number">1</span>)<restore-line-break>    <span class="hljs-keyword">except</span> Exception <span class="hljs-keyword">as</span> e:<restore-line-break>        print(<span class="hljs-string">f"Error processing <span class="hljs-subst">{url}</span>: <span class="hljs-subst">{e}</span>"</span>)<restore-line-break><restore-line-break><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">save_to_xlsx</span>(<span class="hljs-params">results, filename</span>):</span><restore-line-break>    wb = Workbook()<restore-line-break>    ws = wb.active<restore-line-break>    headers = [<restore-line-break>        <span class="hljs-string">"URL"</span>, <span class="hljs-string">"Response Code"</span>, <span class="hljs-string">"Title"</span>, <span class="hljs-string">"Title Length"</span>, <span class="hljs-string">"Meta Description Length"</span>,<restore-line-break>        <span class="hljs-string">"H1 Count"</span>, <span class="hljs-string">"Title Equals First H1"</span>, <span class="hljs-string">"H2 Count"</span>, <span class="hljs-string">"H3 Count"</span>, <span class="hljs-string">"Word Count"</span>,<restore-line-break>        <span class="hljs-string">"HTTPS Links"</span>, <span class="hljs-string">"HTTP Links"</span>, <span class="hljs-string">"Images Without ALT"</span>, <span class="hljs-string">"Images Without Title"</span><restore-line-break>    ]<restore-line-break><restore-line-break>    <span class="hljs-keyword">for</span> col_num, header <span class="hljs-keyword">in</span> enumerate(headers, <span class="hljs-number">1</span>):<restore-line-break>        ws.cell(row=<span class="hljs-number">1</span>, column=col_num, value=header)<restore-line-break><restore-line-break>    <span class="hljs-keyword">for</span> row_num, result <span class="hljs-keyword">in</span> enumerate(results, <span class="hljs-number">2</span>):<restore-line-break>        <span class="hljs-keyword">for</span> col_num, value <span class="hljs-keyword">in</span> enumerate(result, <span class="hljs-number">1</span>):<restore-line-break>            ws.cell(row=row_num, column=col_num, value=value)<restore-line-break><restore-line-break>    wb.save(filename)<restore-line-break><restore-line-break>base_url = input(<span class="hljs-string">"Please enter the base URL: "</span>).strip()<restore-line-break><restore-line-break><span class="hljs-keyword">while</span> <span class="hljs-keyword">not</span> is_valid_url(base_url):<restore-line-break>    print(<span class="hljs-string">"Invalid URL. Please enter a valid URL."</span>)<restore-line-break>    base_url = input(<span class="hljs-string">"Please enter the base URL: "</span>).strip()<restore-line-break><restore-line-break>ignore_patterns = [<span class="hljs-string">r".(jpg|jpeg|png|gif|svg|js|css|ico|mp4|webm|pdf)$"</span>]<restore-line-break><restore-line-break>visited_urls = set()<restore-line-break>results = []<restore-line-break><restore-line-break>crawl(base_url)<restore-line-break><restore-line-break>save_to_xlsx(results, <span class="hljs-string">"url-scan.xlsx"</span>)<restore-line-break><restore-line-break>print(<span class="hljs-string">"URL scan complete. Results saved to 'url-scan.xlsx'."</span>)<restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></restore-line-break></code></pre><p>This script completes the crawling process and saves the results to an "url-scan.xlsx" file. Also, please note that the output file format is xlsx instead of xlst, as xlsx is the correct format for Excel files. The script asks the user for the base URL using the <code>input()</code> function, and checks if it's valid using the <code>is_valid_url()</code> function. If the URL is invalid, it asks the user again until a valid URL is provided.</p></div><footer class="wrapper post__footer"><p class="post__last-updated">This article was updated on June 1, 2024</p><div class="post__share"></div><div class="post__bio bio"><img src="https://itheo.tech/media/website/64.png" loading="lazy" height="512" width="512" class="bio__avatar" alt="Theo van der Sluijs"><div><h3 class="bio__name"><a href="https://itheo.tech/authors/theo-van-der-sluijs/" rel="author">Theo van der Sluijs</a></h3></div></div></footer></article><nav class="post__nav"><div class="post__nav-inner"><div class="post__nav-prev"><svg width="1.041em" height="0.416em" aria-hidden="true"><use xlink:href="https://itheo.tech/assets/svg/svg-map.svg#arrow-prev"/></svg> <a href="https://itheo.tech/get-all-site-urls-without-images-javascript-and-css-files.html" class="post__nav-link" rel="prev"><span>Previous</span> Get all site urls, without images, javascript and css files</a></div><div class="post__nav-next"><a href="https://itheo.tech/masking-a-creditcardnummer-with-python.html" class="post__nav-link" rel="next"><span>Next</span> Masking a creditcardnummer with Python </a><svg width="1.041em" height="0.416em" aria-hidden="true"><use xlink:href="https://itheo.tech/assets/svg/svg-map.svg#arrow-next"/></svg></div></div></nav><div class="post__related related"><div class="wrapper"><h2 class="h5 related__title">You should also read:</h2><article class="related__item"><div class="feed__meta"><time datetime="2018-11-30T23:00" class="feed__date">November 30, 2018</time></div><h3 class="h1"><a href="https://itheo.tech/image-resize-python-automation-script.html">Image resize python automation script</a></h3></article><article class="related__item"><div class="feed__meta"><time datetime="2018-12-11T23:00" class="feed__date">December 11, 2018</time></div><h3 class="h1"><a href="https://itheo.tech/fake-jekyll-posts-generator-script.html">Fake Jekyll posts generator script</a></h3></article><article class="related__item"><div class="feed__meta"><time datetime="2018-12-19T23:00" class="feed__date">December 19, 2018</time></div><h3 class="h1"><a href="https://itheo.tech/get-colors-from-images-python-script.html">Get colors from images python script</a></h3></article></div></div></main><footer class="footer"><div class="footer__copyright"><p>iTheo.Tech</p></div><button onclick="backToTopFunction()" id="backToTop" class="footer__bttop" aria-label="Back to top" title="Back to top"><svg><use xlink:href="https://itheo.tech/assets/svg/svg-map.svg#toparrow"/></svg></button></footer></div><script defer="defer" src="https://itheo.tech/assets/js/scripts.min.js?v=f47c11534595205f20935f0db2a62a85"></script><script>window.publiiThemeMenuConfig={mobileMenuMode:'sidebar',animationSpeed:300,submenuWidth: 'auto',doubleClickTime:500,mobileMenuExpandableSubmenus:true,relatedContainerForOverlayMenuSelector:'.top'};</script><script>var images = document.querySelectorAll('img[loading]');
        for (var i = 0; i < images.length; i++) {
            if (images[i].complete) {
                images[i].classList.add('is-loaded');
            } else {
                images[i].addEventListener('load', function () {
                    this.classList.add('is-loaded');
                }, false);
            }
        }</script></body></html>